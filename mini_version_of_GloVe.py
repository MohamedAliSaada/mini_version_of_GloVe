# -*- coding: utf-8 -*-
"""mini version of GloVe.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZGAiQoS9sLZcsfnvUjSYBFw3nVxNmfPa
"""

#!pip install --upgrade datasets

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from datasets import load_dataset
from tensorflow.keras.preprocessing.text import Tokenizer


bbc_train = load_dataset("SetFit/bbc-news", split="train")
bbc_test = load_dataset("SetFit/bbc-news", split="test")

ag_train = load_dataset("fancyzhx/ag_news", split="train")
ag_test = load_dataset("fancyzhx/ag_news", split="test")

# Combine
corpus = bbc_train["text"] + bbc_test["text"] + ag_train["text"] + ag_test["text"]



# 2. Initialize the tokenizer
tokenizer = Tokenizer(num_words=5000, lower=True, oov_token='<OOV>')
tokenizer.fit_on_texts(corpus)  # Fit on the full corpus

# 3. Define corrected co-occurrence matrix function
def co_occurrence_matrix(corpus, tokenizer, window_size=2):
    sequences = tokenizer.texts_to_sequences(corpus)

    # Limit to the top-k words
    word_index = tokenizer.word_index
    vocab = [word for word, idx in word_index.items() if idx < tokenizer.num_words]
    vocab = sorted(vocab)
    word2idx = {word: i for i, word in enumerate(vocab)}

    # Map from tokenizer index to our reduced vocab index
    idx_map = {word_index[word]: word2idx[word] for word in vocab}

    matrix = np.zeros((len(vocab), len(vocab)), dtype=np.float32)

    for seq in sequences:
        for i, target_id in enumerate(seq):
            if target_id not in idx_map:
                continue
            target_idx = idx_map[target_id]
            start = max(0, i - window_size)
            end = min(len(seq), i + window_size + 1)
            for j in range(start, end):
                if i == j or seq[j] not in idx_map:
                    continue
                context_idx = idx_map[seq[j]]
                matrix[target_idx][context_idx] += 1

    return matrix, vocab, word2idx

# 4. Train GloVe-like embeddings using SVD
def train_glove_embeddings(cooccur_matrix, embedding_dim=100):
    U, S, Vt = np.linalg.svd(cooccur_matrix, full_matrices=False)
    word_vectors = U[:, :embedding_dim] @ np.diag(np.sqrt(S[:embedding_dim]))
    return word_vectors

# 5. Optional plot (for 2D)
def plot_embedding(embeddings, vocab, limit=50):
    plt.figure(figsize=(10, 8))
    for i in range(min(limit, len(vocab))):
        plt.scatter(embeddings[i, 0], embeddings[i, 1], marker='+')
        plt.text(embeddings[i, 0] + .01, embeddings[i, 1] + .01, vocab[i], fontsize=8)
    plt.xlabel("Dimension 1")
    plt.ylabel("Dimension 2")
    plt.title("Word Embeddings")
    plt.grid(True)
    plt.show()

# 6. Cosine similarity function
def similarity(word1, word2, word2idx, embeddings):
    word1 = word1.lower()
    word2 = word2.lower()
    if word1 not in word2idx or word2 not in word2idx:
        return f"One or both words not in vocabulary: {word1}, {word2}"
    vec1 = embeddings[word2idx[word1]].reshape(1, -1)
    vec2 = embeddings[word2idx[word2]].reshape(1, -1)
    return cosine_similarity(vec1, vec2)[0][0]


def save_glove_format(vocab, embeddings, output_file):
    """
    Save embeddings to a .txt file in GloVe format.
    Each line: word followed by embedding values.
    """
    with open(output_file, "w", encoding="utf-8") as f:
        for i, word in enumerate(vocab):
            vector_str = " ".join([f"{val:.6f}" for val in embeddings[i]])
            f.write(f"{word} {vector_str}\n")

# ===== Run the full pipeline =====
co_matrix, vocab, word2idx = co_occurrence_matrix(corpus, tokenizer, window_size=2)
embeddings = train_glove_embeddings(co_matrix, embedding_dim=4)
#save_glove_format(vocab, embeddings, "glove_custom_4d_4999W.txt")





